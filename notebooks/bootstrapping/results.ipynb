{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "554d5eca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpatches\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmpatches\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhierarchical_bootstrapping\u001b[39;00m \u001b[39mimport\u001b[39;00m hierarchical_bootstrapping\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from random import choices\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, average_precision_score\n",
    "from tqdm.notebook import tqdm\n",
    "from src.utils import download_table, get_wandb_summaries\n",
    "from src.data.utils import download_wandb_table\n",
    "import wandb\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "from src.data.hierarchical_bootstrapping import hierarchical_bootstrapping\n",
    "sns.color_palette\n",
    "\n",
    "colors = sns.color_palette().as_hex()\n",
    "print(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ab418",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "\n",
    "def search_experiments(model=\"CNNToTransformerClassifier\", task=\"PredictFluPos\", notes=\"hourly temporal split\", _n_runs=5):\n",
    "    \"\"\"\n",
    "    returns experiments ids that match a given a model name, task, and notes. Should return one entry per run (i.e. 5)\n",
    "    \"\"\"\n",
    "    query = api.runs(\n",
    "                     \n",
    "            path=\"bdata/ICML-2023-Flu-Dataset\",\n",
    "#             path=\"safranchik/mobs\",\n",
    "            filters={\"summary_metrics.model\": model, \"summary_metrics.task\": task, \"notes\": notes},\n",
    "            order=\"+summary_metrics.pl_seed\"\n",
    "    )\n",
    "    if len(query) != 5:\n",
    "        print(\"{}-{} with notes \\\"{}\\\" has length {}\".format(model, task, notes, len(query)))\n",
    "\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be8f1c49",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'roc_auc_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metrics \u001b[39m=\u001b[39m [roc_auc_score, average_precision_score]\n\u001b[1;32m      3\u001b[0m \u001b[39m# format: model name, notes, label, color\u001b[39;00m\n\u001b[1;32m      4\u001b[0m models \u001b[39m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m             (\u001b[39m\"\u001b[39m\u001b[39mCNNToTransformerClassifier\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtemporal split\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mCNN-Transformer\u001b[39m\u001b[39m\"\u001b[39m, colors[\u001b[39m0\u001b[39m]),\n\u001b[1;32m      6\u001b[0m             (\u001b[39m\"\u001b[39m\u001b[39mCNNToTransformerClassifier\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39muser split\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mCNN-Transformer\u001b[39m\u001b[39m\"\u001b[39m, colors[\u001b[39m0\u001b[39m]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m             (\u001b[39m\"\u001b[39m\u001b[39mXGBoostClassifier\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdaily user split\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mXGBoost (daily)\u001b[39m\u001b[39m\"\u001b[39m, colors[\u001b[39m4\u001b[39m])\n\u001b[1;32m     18\u001b[0m         ]            \n",
      "\u001b[0;31mNameError\u001b[0m: name 'roc_auc_score' is not defined"
     ]
    }
   ],
   "source": [
    "metrics = [roc_auc_score, average_precision_score]\n",
    "\n",
    "# format: model name, notes, label, color\n",
    "models = [\n",
    "            (\"CNNToTransformerClassifier\", \"temporal split\", \"CNN-Transformer\", colors[0]),\n",
    "            (\"CNNToTransformerClassifier\", \"user split\", \"CNN-Transformer\", colors[0]),\n",
    "            (\"CNNToTransformerClassifierPretrained\", \"temporal split\", \"CNN-Transformer-Pretrained\", colors[0]),\n",
    "            (\"CNNClassifier\", \"temporal split\", \"CNN\", colors[1]),\n",
    "            (\"CNNClassifier\", \"user split\", \"CNN\", colors[1]),\n",
    "            (\"CNNClassifier\", \"hourly temporal split\", \"CNN (hourly)\", colors[2]),\n",
    "            (\"CNNClassifier\", \"hourly user split\", \"CNN (hourly)\", colors[2]),\n",
    "          \n",
    "            (\"TransformerClassifier\", \"hourly temporal split\", \"Transformer (hourly)\", colors[3]),\n",
    "#             (\"TransformerClassifier\", \"hourly user split\", \"Transformer (hourly)\", colors[3]),          \n",
    "          \n",
    "            (\"XGBoostClassifier\", \"daily temporal split\", \"XGBoost (daily)\", colors[4]),\n",
    "            (\"XGBoostClassifier\", \"daily user split\", \"XGBoost (daily)\", colors[4])\n",
    "        ]            \n",
    "\n",
    "tasks = [\"HomekitPredictFluPos\", \"HomekitPredictFatigue\", \"HomekitPredictCough\"]\n",
    "task_names = [\"Flu Pos\", \"Fatigue\", \"Cough\"]\n",
    "assert len(tasks) == len(task_names)\n",
    "\n",
    "splits = [\"user\", \"temporal\"]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2, 2)\n",
    "fig.set_size_inches(20, 10)\n",
    "\n",
    "# width of bar charts\n",
    "width = 0.2\n",
    "\n",
    "def get_prediction_tables(run_ids):\n",
    "    tables = []\n",
    "    for run_id in run_ids:\n",
    "        try:\n",
    "#         tables.append(download_wandb_table(run_id.id, table_name=\"test_predictions\", entity=\"safranchik/bdata\", project=\"ICML-2023-Flu-Dataset\"))\n",
    "            # TODO: pull runs from the bdata/Homekit project \n",
    "            tables.append(download_wandb_table(run_id.id, table_name=\"test_predictions\", entity=\"bdata\", project=\"ICML-2023-Flu-Dataset\"))    \n",
    "        except:\n",
    "            continue\n",
    "    return tables\n",
    "\n",
    "\n",
    "# download_wandb_table(run_id.id, table_name=\"test_predictions\", entity=\"bdata\", project=\"ICML-2023-Flu-Dataset/runs\")\n",
    "\n",
    "\n",
    "data = {\"temporal\": {\"roc_auc_score\": {}, \"average_precision_score\": {}},\n",
    "        \"user\": {\"roc_auc_score\": {}, \"average_precision_score\": {}}}\n",
    "\n",
    "\"\"\" fills out the nested data dictionary \"\"\"\n",
    "for j, task in enumerate(tasks):\n",
    "    for k, (model, note, label, _) in enumerate(models):\n",
    "\n",
    "        split = \"temporal\" if \"temporal\" in note else \"user\"\n",
    "        \n",
    "        run_ids = search_experiments(model, task, note)\n",
    "        tables = get_prediction_tables(run_ids)\n",
    "\n",
    "        if not isinstance(tables, float) and not len(tables):\n",
    "            continue\n",
    "            \n",
    "        # TODO: change model bootstraps to 40 and data bootstraps to 10\n",
    "        # 40 model bootstraps lets us discard the bottom and top outliers to obtain a 95% CI\n",
    "        bootstraps = hierarchical_bootstrapping(tables, num_bootstraps=10, metrics=metrics)\n",
    "            \n",
    "        for l, metric in enumerate(metrics):\n",
    "\n",
    "            bootstraps_list = bootstraps[metric.__name__]\n",
    "            metric_mean = np.mean(bootstraps_list)\n",
    "            metric_err = np.abs(np.percentile(bootstraps_list, (0.975, 0.025)) - metric_mean)\n",
    "\n",
    "            model_name = model + \"_({})\".format(note)\n",
    "            if model_name not in data[split][metric.__name__]:\n",
    "                data[split][metric.__name__][model_name] = {t: (0,(0,0)) for t in tasks}\n",
    "                \n",
    "            data[split][metric.__name__][model_name][task] = (metric_mean, metric_err)\n",
    "         \n",
    "\"\"\" plots the nested data dictionary \"\"\"\n",
    "for i, split in enumerate(splits):\n",
    "    for l, metric in enumerate(metrics):\n",
    "        \n",
    "        ticks = [j + (len(tasks)-1) * width / 2 for j in range(len(tasks))]\n",
    "\n",
    "        ax[i, l].set_xticks(ticks=ticks, labels=task_names)\n",
    "        ax[i, l].set_title(\"{} split ({})\".format(split, metric.__name__))\n",
    "        ax[i, l].grid(axis='y', which=\"both\")\n",
    "\n",
    "        for k, (model, note, label, color) in enumerate(models):\n",
    "            model_name = model + \"_({})\".format(note)\n",
    "\n",
    "            if model_name in data[split][metric.__name__]:\n",
    "                \n",
    "                task_scores = [data[split][metric.__name__][model_name][t][0] for t in tasks]\n",
    "\n",
    "                lower_err = [data[split][metric.__name__][model_name][t][1][0] for t in tasks]\n",
    "                upper_err = [data[split][metric.__name__][model_name][t][1][1] for t in tasks]\n",
    "\n",
    "                x = np.arange(len(tasks)) + k * width\n",
    "                ax[i, l].bar(x=x, height=task_scores, width=width, yerr=(upper_err, lower_err), color=color)\n",
    "\n",
    "            \n",
    "fig.tight_layout()\n",
    "\n",
    "handles = [mpatches.Patch(label=m[0], color=m[-1]) for m in models]\n",
    "labels = [m[2] for m in models]\n",
    "fig.legend(handles, labels)\n",
    "fig_name = \"big_bar_plot.pdf\"\n",
    "fig.savefig(fig_name, bbox_inches='tight', format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a03e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a6b415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) \n[GCC 10.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "a206d40103c55017034b7e8b8a7d2fc1a21b82b23421476961c8bdc7d4e24620"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
